\documentclass[11pt]{article}
\usepackage{geometry}               
\geometry{a4paper}                

\bibliographystyle{plain}

\title{Statistical Structures in Language Processing - Phrase Extraction}
\author{Anouk Visser \& R\'emi de Zoeten}
\date{}                                           % Activate to display a given date or no date

\begin{document}

\maketitle
\newpage
\tableofcontents
\newpage

\section{abstract}
\section{introduction}
\section{Phrase Extraction and Phrase Translation Probabilites}
Phrase translation probabilities play an important role in phrase-based machine translation systems \cite{koehn}. To be able to compute the joint and (bidirectional) conditional translation probabilities of a phrase pair, we first need to extract all possible phrase pairs from an aligned training corpus. Although many phrase pairs can be extracted from two parallel sentences, there are only a few that are likely to be each other's translation. In order to limit the number of phrase pairs we extract from the data, we only want to extract phrases that are consistent with the word alignments. A phrase pair is consistent with a word alignment if all words in the source phrase align with a word in the target phrase and vice versa.  In addition to allowing only consistent phrase pairs we will further restrict ourselves to extracting only phrases up to length 4. In section \ref{implem1} and \ref{implem3} we give an overview of how we extracted the phrase pairs and computed their joint and (bidirectional) translation probabilities.\\\\
Once we obtain a phrase table, we might wonder how the extracted phrases of one parallel corpus compare to the phrases extracted from another parallel corpus. In order to explore the coverage of the phrase table extracted from the training data we compute the percentage of phrase pairs in a held-out set which are available in the training set phrase table. We could simply look at how may of the phrase pairs are available in the training set phrase table directly or we could concatenate a number of phrase pairs from the training set to retrieve the phrase pairs in the held-out set. In section \ref{implem2} we will discuss both methods, in \ref{eval} we discuss our results.
%Build an efficient tool for extracting all phrase pairs up to length 4 from a given word aligned training parallel corpus with their joint and conditional (two directional) probability estimates. 
%Explore the coverage (sparsity) of the phrase table by computing the percentage phrase pairs in a held-out set which 
%available in the training set phrase table
%an be built by concatenating (in any order) n>0 phrase pairs from the training set phrase table: it is important to set an upper-bound on n, E.g., n<=3 which makes this more reasonable to execute
%Given the training data of 100K bilingual sentences, build a phrase-based SMT system using the Moses installation 
%calculate BLEU scores of the test set  \cite{anouk}

\section{Related work}
In phrase-based machine translation systems we attempt to find an English sentence so that: 
$$\textit{argmax}_e P(e) \times P(f|e)$$
To compute $P(f|e)$, in a phrase-based machine translation system we use phrases instead of words. Different methods have been proposed to extract phrase pairs from a parallel corpus in order to estimate the phrase translation probabilities. A very common method is to extract phrase pairs from a word alignment \cite{koehn}\cite{theother}. In this method all phrases for both sentences are considered, but only the phrase pairs that are consistent with the word alignment are extracted. A phrase pair $(f\bar, e\bar)$ is consistent with the word alignment $A$ if
\begin{enumerate}
\item at least one pair from the alignment occurs in the phrase pair
\item all words in the foreign phrase aligned to a word in the target phrase or not aligned at all
\item all words in the target phrase are aligned to a word in the source phrase.
\end{enumerate} 
This method of extracting phrases is closest to our implementation. In \cite{super} a method is proposed based on inversion transduction grammars that reduces the alignment and the extraction step into a single step. \\\\
There are different ways of estimating the phrase translation probabilities, the simplest way is to calculate the relative frequency. However, other methods have been proposed such as lexical weighting \cite{lexical} where the phrase translation probabilities are estimated by using the conditional probabilities of the words that make up the phrase.
% also sort of 'possible solutions', right?

\section{Implementation}
\label{implem}
In this section we will describe our implementation for the phrase extraction algorithm, as well as the probability estimation algorithm and the algorithm to compute the coverage of the phrase table. 
\subsection{Phrase Extraction}
\label{implem1}

\subsection{Probability Estimation}
\label{implem2}
We extracted the frequencies of all phrases $p$ in both languages and also the cooccurrences of $f$ and $e$.
This gave us two frequency tables, \textit{freq\_e} and \textit{freq\_f} and two cooccurrence matrices, \textit{coc\_e\_f} and \textit{coc\_f\_e} where \textit{coc\_e\_f} can be used to look up how often each foreign phrase $f$ cooccurs with a given phrase $e$. 

These tables can be used to estimate the following probabilities:

$$ p(f) = \frac{freq\_f[f] }{  \sum_{f_i \in freq\_f} freq\_f[f_i]  } $$
$$ p(e) = \frac{freq\_e[e] }{  \sum_{e_i \in freq\_e} freq\_e[e_i]  } $$
$$ p(f|e) = \frac{coc\_e\_f[e][f]}{ \sum_{f_i \in coc\_e\_f[e]}  coc\_e\_f[e][f_i]} $$
$$ p(e|f) = \frac{coc\_f\_e[f][e]}{ \sum_{e_i \in coc\_f\_e[f]}  coc\_f\_e[f][e_i]} $$
$$ p(e, f) = p(e|f) \times p(f) $$
$$ p(e, f) = p(f|e) \times p(e) $$

\subsection{Coverage}
\label{implem2}


\subsection{Efficiency}

% selected solution
%\section{implementation issues}
\section{Experimental Setting}

\subsection{BLEU stuff}

\subsection{Empirical Evaluation and Coverage}
\label{eval}

\section{Results}

\subsection{BLEU stuff}

\subsection{Empirical Evaluation and Coverage}

\section{conclusion}
\bibliography{test}
\end{document}  