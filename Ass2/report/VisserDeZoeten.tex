\documentclass[11pt]{article}
\usepackage{geometry} 
\usepackage{graphicx}
\usepackage{float}
\geometry{a4paper}                

\bibliographystyle{plain}

\title{Statistical Structures in Language Processing - Phrase Extraction}
\author{Anouk Visser \& R\'emi de Zoeten}
\date{} 

\begin{document}

\maketitle
\newpage
\tableofcontents
\newpage

\section{Abstract}
In this paper we present our findings from Statistical Machine Translation lab session 2. We built an efficient tool for extracting all phrase pairs from an aligned corpus and show how this extraction can be used to derive some probabilities. We were also requested to perform two types of phrase coverage analyses, of which one was successful. We were also able to train the moses program on a large aligned corpus, although we could not analyse the resulting BLEU performance.

\section{Introduction}

\section{Phrase Extraction}
\label{problem}
Phrase translation probabilities play an important role in phrase-based machine translation systems \cite{koehn}. To be able to compute the joint and (bidirectional) conditional translation probabilities of a phrase pair, we first need to extract all possible phrase pairs from an aligned training corpus. Although many phrase pairs can be extracted from two parallel sentences, there are only a few that are likely to be each other's translation. In order to limit the number of phrase pairs we extract from the data, we only want to extract phrases that are consistent with the word alignments. A phrase pair is consistent with a word alignment if all words in the source phrase align with a word in the target phrase and vice versa.  In addition to allowing only consistent phrase pairs we will further restrict ourselves to extracting only phrases up to length 4. In section \ref{implem1} and \ref{implem3} we give an overview of how we extracted the phrase pairs and computed their joint and (bidirectional) translation probabilities.\\\\
Once we obtain a phrase table, we might wonder how the extracted phrase table of one parallel corpus compare to the phrase table extracted from another parallel corpus. In order to explore the coverage of the phrase table extracted from the training data we compute the percentage of phrase pairs in a held-out set which are available in the training set phrase table. We could simply look at the percentage of phrase pairs directly available in the training set phrase table or we could concatenate a number of phrase pairs from the training set to retrieve the phrase pairs in the held-out set. In section \ref{implem2} we will discuss both methods, in \ref{eval} we discuss our results.
%Build an efficient tool for extracting all phrase pairs up to length 4 from a given word aligned training parallel corpus with their joint and conditional (two directional) probability estimates. 
%Explore the coverage (sparsity) of the phrase table by computing the percentage phrase pairs in a held-out set which 
%available in the training set phrase table
%an be built by concatenating (in any order) n>0 phrase pairs from the training set phrase table: it is important to set an upper-bound on n, E.g., n<=3 which makes this more reasonable to execute
%Given the training data of 100K bilingual sentences, build a phrase-based SMT system using the Moses installation 
%calculate BLEU scores of the test set  \cite{anouk}

\section{Related work}
In phrase-based machine translation systems we attempt to find an English sentence so that: 
$$\textit{argmax}_e P(e) \times P(f|e)$$
To compute $P(f|e)$, in a phrase-based machine translation system we use phrases instead of words. Different methods have been proposed to extract phrase pairs from a parallel corpus in order to estimate the phrase translation probabilities. A very common method is to extract phrase pairs from a word alignment \cite{koehn}\cite{theother}. In this method all phrases for both sentences are considered, but only the phrase pairs that are consistent with the word alignment are extracted. A phrase pair $(f\bar, e\bar)$ is consistent with the word alignment $A$ if
\begin{enumerate}
\item at least one pair from the alignment occurs in the phrase pair
\item all words in the foreign phrase aligned to a word in the target phrase or not aligned at all
\item all words in the target phrase are aligned to a word in the source phrase.
\end{enumerate} 
This method of extracting phrases is closest to our implementation. In \cite{super} a method is proposed based on inversion transduction grammars that reduces the alignment and the extraction step into a single step. \\\\
There are different ways of estimating the phrase translation probabilities, the simplest way is to calculate the relative frequency. However, other methods have been proposed such as lexical weighting \cite{lexical} where the phrase translation probabilities are estimated by using the conditional probabilities of the words that make up the phrase.
% also sort of 'possible solutions', right?

\section{Implementation}
In this section we will describe our implementation for the phrase extraction algorithm, as well as the probability estimation algorithm and the algorithm to compute the coverage of the phrase table. 

\subsection{Phrase Extraction}
\label{implem1}
Our algorithm first extracts all possible phrases up to length 4 from both the source as well as the target sentence. For every combination of the extracted phrases it will then check whether or not this phrase pair is consistent with the given word alignment. It does so by collecting all the words with which the source sentence as well as the target sentence are aligned. Let $e$ be the target phrase and $f$ be the source phrase. We check for every word in $e$ to which word in $f$ it is aligned according to the given alignment and call the set of found alignments $e_a$. Note that this set corresponds directly to the words in $f$. We do the same for all words in $f$ giving us the set $f_a$. Because $f_a$ and $e_a$ correspond directly to $e$ and $f$ respectively we can determine whether the phrase pair is consistent with the given word alignment by checking if $f_a$ is a subset of $e$ and $e_a$ is a subset of $f$. If this is the case, we conclude that all words in the foreign phrase are aligned to a word in the target phrase (or not aligned at all) and all words in the target phrase are aligned to a word in the source phrase, covering two of the three requirements for consistency. None of the words from either side is aligned to a word that is not in one of the phrase pairs. The last requirement is most easy to meet, at least one word in $f$ as well as $e$ has to be aligned with another word.

\subsection{Probability Estimation}
\label{implem2}
We extracted the frequencies of all phrases $p$ in both languages and also the cooccurrences of $f$ and $e$.
This gave us two frequency tables, \textit{freq\_e} and \textit{freq\_f} and two cooccurrence matrices, \textit{coc\_e\_f} and \textit{coc\_f\_e} where \textit{coc\_e\_f} can be used to look up how often each foreign phrase $f$ cooccurs with a given phrase $e$. 

These tables can be used to estimate the following probabilities:

$$ p(f) = \frac{freq\_f[f] }{  \sum_{f_i \in freq\_f} freq\_f[f_i]  } $$
$$ p(e) = \frac{freq\_e[e] }{  \sum_{e_i \in freq\_e} freq\_e[e_i]  } $$
$$ p(f|e) = \frac{coc\_e\_f[e][f]}{ \sum_{f_i \in coc\_e\_f[e]}  coc\_e\_f[e][f_i]} $$
$$ p(e|f) = \frac{coc\_f\_e[f][e]}{ \sum_{e_i \in coc\_f\_e[f]}  coc\_f\_e[f][e_i]} $$
$$ p(e, f) = p(e|f) \times p(f) $$
$$ p(e, f) = p(f|e) \times p(e) $$

\subsection{Coverage}
\label{implem2}
As described in section \ref{problem} there are two ways of computing the coverage of the phrase table extracted from the training data:
\begin{enumerate}
\item compute the percentage of phrase pairs in the held-out phrase table that are directly accessible in the phrase table extracted from the training data
\item compute the percentage of phrase pairs in the held-out phrase table that can be found by concatenating up to $n$ phrase pairs from the phrase table extracted from the training data.
\end{enumerate}
The first method is straightforward, so we have tried to come up with an algorithm for computing the coverage that uses the second method. \\\
Our algorithm is illustrated in figure \ref{algorithm}, to keep things simple we will just focus on one phrase, but this algorithm can be easily extended to two phrases. For every phrase in the held-out phrase table (far right) it will find all phrases in the phrase table extracted from the training data (left) that are a subsequence of the phrase in the held-out phrase table and contain only words that are in the held-out phrase, `the murderer' and `Sherlock and John' do not meet this criteria and are therefore discarded (`x'). For the remaining phrases we find the `span' in the phrase that we are looking for. For example `John and Sherlock solve' are the 0th, 1st, 2nd and 3th word of the phrase that we are trying to concatenate, thus resulting in span `0-3'. \\\\
Our algorithm starts searching for spans that start with a 0 and so it will find `John and Sherlock' and `John and Sherlock solve' first. Next, for every span it has found it will look for the next possible `building blocks' until it has found the goal, which is a span that ends with the length of the phrase we are looking for (-1). For  `John and Sherlock' all spans starting with 3 are considered, the only span that we can find is 3-5 which brings us to our goal. After concatenating $n$ `building blocks' our algorithm abandons its search (in our experiments we set $n=3$).

\begin{figure}
    \begin{tabular}{l|l|l}
    John and Sherlock       & 0 - 2 & John and Sherlock solve a crime \\
    the murderer            & x     & ...                             \\
    a crime                 & 4 - 5 & ...                             \\
    solve a crime           & 3 - 5 & ...                             \\
    Sherlock solve          & 2 - 3 & ~                               \\
    John and Sherlock solve & 0 - 3 & ~                               \\
    Sherlock and John       & x     & ~                               \\
    \end{tabular}
\caption{An illustrative example of the coverage-concatenation algorithm.}
    \label{algorithm}
\end{figure}

% selected solution
%\section{implementation issues}
\section{Experimental Setting}

\subsection{BLEU stuff}

\subsection{Empirical Evaluation and Coverage}
\label{eval}

\section{Results}
We ran our phrase extraction on the smaller 'heldout' dataset and measured how many phrases were extracted and stored in the four different tables that were described in section \ref{implem2}. The following table shows the results:

\begin{figure}[H]
    \begin{tabular}{l|l|l|l}
    freq\_e  & freq\_f & cocs\_e\_f  & cocs\_f\_e \\
    117835 & 112893 & 52664 & 49385 \\
    \end{tabular}
\caption{Phrase counts}
\end{figure}


\section{Conclusion}

\bibliography{test}

\end{document}  