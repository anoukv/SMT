%
% File acl2012.tex
%
% Contact: Maggie Li (cswjli@comp.polyu.edu.hk), Michael White (mwhite@ling.osu.edu)
%%
%% Based on the style files for ACL2008 by Joakim Nivre and Noah Smith
%% and that of ACL2010 by Jing-Shin Chang and Philipp Koehn


\documentclass[11pt]{article}
\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Instructions for ACL2012 Proceedings}

\author{Anouk Visser \\
  Affiliation / Address line 1 \\
  {\tt email@domain} \\\And
  R\'emi de Zoeten \\
  Affiliation / Address line 1 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle

\section{Introduction}
Intro


\section{IBM Model I}
IBM model 1 is the first and least complex in the series of IBM translation models. It is based only on word level translation probabilities that are derived from a training corpus. The model only defines translation probabilities as $P(f|e)$ where $f$ is a french word and $e$ is an english word and does not look at word context or sentence structure. The $P(f|e)$ is obtained using the expectation maximisation algorithm described in \ref{em}. IBM model 1 does not address word re-ordering or sentence structure.

\section{Expectation maximisation training formula}
\label{em}
The expectation maximisation (em) algorithm is used to iteratively re-estimate the probability of $P(f|e)$, converging on every iteration. The estimated probabilities $P(f|e)$ are initialised uniformly and are stored in a translation table $T$

\section{Improvements}

\section{Results}




\begin{thebibliography}{}

\bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
Alfred~V. Aho and Jeffrey~D. Ullman.
\newblock 1972.
\newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
\newblock Prentice-{Hall}, Englewood Cliffs, NJ.


\end{thebibliography}

\end{document}
