\documentclass[11pt]{article}
\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\usepackage{float}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{IBM Model 1}

\author{Anouk Visser \\
  {\tt anouk.visser@student.uva.nl} \\\And
  R\'emi de Zoeten \\
  {\tt remi.de.z@gmail.com} \\}

\date{}

\begin{document}
\maketitle

\section{Introduction}
One solution to machine translation is to find translations of words in the sentence. IBM Model 1 \cite{IBM} is an example of such a word-based model. In this report we present our implementation of IBM Model 1 and its Expectation-Maximization (EM) training. In section \ref{IBM} we will give a little background on IBM Model 1, in section \ref{em} we lay out the EM formulas. In section \ref{own} we propose a simple improvement to IBM Model 1 reducing its assumptions. Finally, we present our results in section \ref{results}.


\section{IBM Model I}
\label{IBM}
IBM model 1 is the first and least complex in the series of IBM translation models. It is based only on word level translation probabilities that are derived from a training corpus. The model only defines translation probabilities as $P(f|e)$ where $f$ is a french word and $e$ is an english word and does not look at word context or sentence structure. The $P(f|e)$ is obtained using the EM algorithm described in section \ref{em}. IBM model 1 does not address word re-ordering or sentence structure.

\section{Expectation Maximization Training Formula}
\label{em}
The Expectation-Maximization (EM) algorithm is used to iteratively re-estimate the probability of $P(f|e)$, converging on every iteration. The estimated probabilities $P(f|e)$ are initialized uniformly and are stored in a translation table $T$. By first applying the current model to the data (starting with uniform) we can get an expectation (E-step), we can use this information to learn the model from the actual data (M-step).\\\\
The E-step is performed separately for every sentence pair. We iterate over all possible alignments to find the probability of an alignment given the source and the target by looking up the probability for the required translations (for that specific alignment) in the translation table and normalizing the following formula: 
$$P_t(a|f, e) = \prod\limits_{j=1}^{m} (\frac{t(f_j|e_{a_{j}})}{\sum\limits_{i=0}^{l}t(f_j|e_i)})$$
The M-step re-estimates the translation probabilities by weighing the counts of the number of times an alignment occurred by its probability and normalizing this. 
$$t(f|e) = \frac{\sum\limits_{(\textbf{e}, \textbf{f})} c(e|f; \textbf{e}, \textbf{f})}{\sum\limits_{e}\sum\limits_{(\textbf{e}, \textbf{f})} c(e|f; \textbf{e}, \textbf{f})}$$

\section{Improvements}
\label{own}
The expectation maximization algorithm needs to run many iterations, which can be time consuming. In the original IBM model the probabilities for all $P(f|e)$ stored in $T$ are initialized uniformly. Our intuition was that there is a better way to initialize it. When initializing the probabilities one has to loop over the corpus and observe all pairs $(f,e)$ and place them in the table. What we did was assign a higher probability to pairs that are observed more frequently, because they can be considered more likely translation pairs for each other. So instead of initializing with a uniform probability, we initialized with a probability proportional to the relative frequency in the data, considering every possible alignment. Doing this requires no extra computations. We expect that this initialization gives higher precision and recall than uniform initialization when performing the same amount of expectation maximization algorithms. In section \ref{results} we present our results.

\section{Results}
\label{results}
We tested our results on the $1000$ sentences that were provided to us. We performed two tests with 3 and 20 expectation maximization iterations over the data and calculated the average recall and precision for all sentences in the data. `Uniform' indicates a uniform initialization of the translation table $T$, `Frequency' indicates an initialization proportional to the frequency.

\begin{table}[H]
    \begin{tabular}{lll}
    ~ & recall  & precision \\
    Uniform & 0.863 & 0.863 \\
    Frequency & 0.883 & 0.883 \\
    \end{tabular}
    \caption{Performance score after 3 EM iterations}
\end{table}

\begin{table}[H]
    \begin{tabular}{lll}
    ~ & recall  & precision \\
    Uniform & 0.930 & 0.930 \\
    Frequency & 0.932 & 0.932 \\
    \end{tabular}
    \caption{Performance score after 20 EM iterations}
\end{table}

\section{Conclusion}
We succeeded in implementing IBM model 1 and reported precision and recall for the training data. We made a small improvement to the EM algorithm by applying a heuristic during initialization. The improvement assures that the EM algorithm converges faster while having no additional computational costs.
\begin{thebibliography}{}

\bibitem[\protect\citename{Brown \bgroup \em et al.\egroup }{1993a}]{IBM}
Brown, P.~F., V.~J.~Della Pietra, S.~A.~Della Pietra, and R.~L. Mercer. 1993.
\newblock The mathematics of statistical machine translation: Parameter
  estimation.
\newblock {\em Computational Linguistics} 19(2).


\end{thebibliography}

\end{document}
